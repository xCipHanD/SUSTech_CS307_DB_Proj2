#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CS307æ•°æ®åº“é¡¹ç›® - æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
ä¸“é—¨ç”¨äºæµ‹è¯•æ•°æ®åº“çš„æ€§èƒ½æŒ‡æ ‡å’ŒåŸºå‡†
"""

import requests
import json
import time
import random
import string
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple
import urllib.parse
import matplotlib.pyplot as plt
import pandas as pd
from dataclasses import dataclass


@dataclass
class PerformanceResult:
    test_name: str
    operation_type: str
    record_count: int
    execution_time: float
    throughput: float  # operations per second
    success: bool
    error_message: str = None


class PerformanceBenchmark:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.results: List[PerformanceResult] = []
        self.test_data_created = False

    def log(self, message: str, level: str = "INFO"):
        """æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")

    def execute_sql(
        self, sql: str, timeout: int = 60
    ) -> Tuple[Dict[str, Any], float, bool]:
        """æ‰§è¡ŒSQLè¯­å¥å¹¶è¿”å›ç»“æœ"""
        start_time = time.time()
        try:
            encoded_sql = urllib.parse.quote(sql)
            response = requests.get(
                f"{self.base_url}?sql={encoded_sql}", timeout=timeout
            )
            execution_time = time.time() - start_time

            try:
                result = response.json()
                success = response.status_code == 200 and result.get("status") in [
                    "success",
                    "help",
                ]
                return result, execution_time, success
            except json.JSONDecodeError:
                return (
                    {"status": "error", "message": "Invalid JSON"},
                    execution_time,
                    False,
                )

        except Exception as e:
            execution_time = time.time() - start_time
            return {"status": "error", "message": str(e)}, execution_time, False

    def create_test_tables(self):
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•æ‰€éœ€çš„è¡¨"""
        self.log("åˆ›å»ºæ€§èƒ½æµ‹è¯•è¡¨...")

        tables = [
            """CREATE TABLE perf_small (
                id INTEGER,
                name CHAR(32),
                value FLOAT,
                category INTEGER
            );""",
            """CREATE TABLE perf_medium (
                id INTEGER,
                title CHAR(64),
                content CHAR(128),
                score DOUBLE,
                created_at CHAR(20)
            );""",
            """CREATE TABLE perf_large (
                student_id INTEGER,
                student_name CHAR(64),
                course_id INTEGER,
                course_name CHAR(64),
                grade FLOAT,
                semester CHAR(16)
            );""",
            """CREATE TABLE perf_index_test (
                pk_id INTEGER,
                indexed_col INTEGER,
                search_field CHAR(32),
                data_payload CHAR(100)
            );""",
        ]

        for i, sql in enumerate(tables, 1):
            result, exec_time, success = self.execute_sql(sql)
            if success:
                self.log(f"âœ… åˆ›å»ºæµ‹è¯•è¡¨ {i}/4 æˆåŠŸ")
            else:
                self.log(
                    f"âŒ åˆ›å»ºæµ‹è¯•è¡¨ {i}/4 å¤±è´¥: {result.get('message', 'Unknown error')}",
                    "ERROR",
                )
                return False

        self.test_data_created = True
        return True

    def generate_test_data(self, table_name: str, record_count: int) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        data = []

        if table_name == "perf_small":
            for i in range(record_count):
                name = f"item_{i:06d}"
                value = round(random.uniform(0, 1000), 2)
                category = random.randint(1, 10)
                data.append(f"({i}, '{name}', {value}, {category})")

        elif table_name == "perf_medium":
            for i in range(record_count):
                title = f"Title_{i:06d}_{''.join(random.choices(string.ascii_letters, k=10))}"
                content = f"Content_{''.join(random.choices(string.ascii_letters + string.digits, k=20))}"
                score = round(random.uniform(0, 100), 3)
                created_at = f"2024-01-{(i % 28) + 1:02d}"
                data.append(f"({i}, '{title}', '{content}', {score}, '{created_at}')")

        elif table_name == "perf_large":
            courses = [
                "Math",
                "Physics",
                "Chemistry",
                "Biology",
                "History",
                "English",
                "Computer Science",
            ]
            semesters = ["2023Fall", "2024Spring", "2024Fall"]

            for i in range(record_count):
                student_name = (
                    f"Student_{''.join(random.choices(string.ascii_letters, k=8))}"
                )
                course_id = random.randint(1, 100)
                course_name = random.choice(courses)
                grade = round(random.uniform(60, 100), 2)
                semester = random.choice(semesters)
                data.append(
                    f"({i}, '{student_name}', {course_id}, '{course_name}', {grade}, '{semester}')"
                )

        elif table_name == "perf_index_test":
            for i in range(record_count):
                indexed_col = i % 1000  # åˆ›å»ºé‡å¤å€¼ä»¥æµ‹è¯•ç´¢å¼•æ•ˆæœ
                search_field = f"search_{i % 100:03d}"
                payload = (
                    f"payload_{''.join(random.choices(string.ascii_letters, k=20))}"
                )
                data.append(f"({i}, {indexed_col}, '{search_field}', '{payload}')")

        return data

    def benchmark_insert_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šæ’å…¥æ€§èƒ½"""
        self.log("=== æ’å…¥æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        test_configs = [
            ("å°è¡¨å•æ¡æ’å…¥", "perf_small", 100, 1),
            ("å°è¡¨æ‰¹é‡æ’å…¥", "perf_small", 1000, 50),
            ("ä¸­è¡¨æ‰¹é‡æ’å…¥", "perf_medium", 500, 25),
            ("å¤§è¡¨æ‰¹é‡æ’å…¥", "perf_large", 1000, 100),
        ]

        for test_name, table_name, total_records, batch_size in test_configs:
            self.log(f"è¿è¡Œæµ‹è¯•: {test_name}")

            total_time = 0
            successful_inserts = 0

            for start_idx in range(0, total_records, batch_size):
                end_idx = min(start_idx + batch_size, total_records)
                batch_data = self.generate_test_data(table_name, end_idx - start_idx)

                # è°ƒæ•´IDä»¥é¿å…é‡å¤
                adjusted_data = []
                for i, data_str in enumerate(batch_data):
                    new_id = start_idx + i + 1000  # åç§»é¿å…å†²çª
                    adjusted_data.append(data_str.replace(f"({i},", f"({new_id},", 1))

                sql = f"INSERT INTO {table_name} VALUES {', '.join(adjusted_data)};"

                result, exec_time, success = self.execute_sql(sql)
                total_time += exec_time

                if success:
                    successful_inserts += len(adjusted_data)
                else:
                    self.log(
                        f"æ‰¹é‡æ’å…¥å¤±è´¥: {result.get('message', 'Unknown error')}",
                        "ERROR",
                    )

            throughput = successful_inserts / total_time if total_time > 0 else 0

            perf_result = PerformanceResult(
                test_name=test_name,
                operation_type="INSERT",
                record_count=successful_inserts,
                execution_time=total_time,
                throughput=throughput,
                success=successful_inserts > 0,
            )

            self.results.append(perf_result)
            self.log(
                f"âœ… {test_name}: {successful_inserts} æ¡è®°å½•, {total_time:.3f}s, {throughput:.2f} ops/sec"
            )

    def benchmark_select_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šæŸ¥è¯¢æ€§èƒ½"""
        self.log("=== æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        queries = [
            ("å…¨è¡¨æ‰«æå°è¡¨", "SELECT * FROM perf_small;"),
            ("æ¡ä»¶æŸ¥è¯¢å°è¡¨", "SELECT * FROM perf_small WHERE category = 5;"),
            (
                "èšåˆæŸ¥è¯¢å°è¡¨",
                "SELECT COUNT(*), AVG(value), MAX(value) FROM perf_small;",
            ),
            ("å…¨è¡¨æ‰«æä¸­è¡¨", "SELECT * FROM perf_medium;"),
            ("æ¡ä»¶æŸ¥è¯¢ä¸­è¡¨", "SELECT * FROM perf_medium WHERE score > 80;"),
            ("æ’åºæŸ¥è¯¢ä¸­è¡¨", "SELECT * FROM perf_medium ORDER BY score DESC;"),
            (
                "å¤æ‚æ¡ä»¶æŸ¥è¯¢",
                "SELECT * FROM perf_large WHERE grade > 85 AND course_id < 50;",
            ),
            (
                "JOINæŸ¥è¯¢",
                """
                SELECT p1.student_name, p2.title 
                FROM perf_large p1 
                JOIN perf_medium p2 ON p1.student_id = p2.id 
                LIMIT 100;
            """,
            ),
        ]

        for test_name, sql in queries:
            # è¿è¡Œå¤šæ¬¡å–å¹³å‡å€¼
            times = []
            for _ in range(5):
                result, exec_time, success = self.execute_sql(sql)
                if success:
                    times.append(exec_time)
                else:
                    self.log(
                        f"æŸ¥è¯¢å¤±è´¥ {test_name}: {result.get('message', 'Unknown error')}",
                        "ERROR",
                    )
                    break

            if times:
                avg_time = statistics.mean(times)
                throughput = 1 / avg_time if avg_time > 0 else 0

                perf_result = PerformanceResult(
                    test_name=test_name,
                    operation_type="SELECT",
                    record_count=1,  # å•æ¬¡æŸ¥è¯¢
                    execution_time=avg_time,
                    throughput=throughput,
                    success=True,
                )

                self.results.append(perf_result)
                self.log(
                    f"âœ… {test_name}: å¹³å‡ {avg_time:.3f}s ({min(times):.3f}-{max(times):.3f}s)"
                )

    def benchmark_update_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šæ›´æ–°æ€§èƒ½"""
        self.log("=== æ›´æ–°æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        updates = [
            ("å•æ¡æ›´æ–°", "UPDATE perf_small SET value = 999.99 WHERE id = 1001;"),
            ("æ‰¹é‡æ›´æ–°", "UPDATE perf_small SET category = 99 WHERE category <= 5;"),
            (
                "æ¡ä»¶æ›´æ–°",
                "UPDATE perf_medium SET score = score * 1.1 WHERE score < 70;",
            ),
            ("å…¨è¡¨æ›´æ–°", "UPDATE perf_small SET name = CONCAT(name, '_updated');"),
        ]

        for test_name, sql in updates:
            result, exec_time, success = self.execute_sql(sql)

            if success:
                throughput = 1 / exec_time if exec_time > 0 else 0
                affected_rows = result.get("affected_rows", 0)

                perf_result = PerformanceResult(
                    test_name=test_name,
                    operation_type="UPDATE",
                    record_count=affected_rows,
                    execution_time=exec_time,
                    throughput=throughput,
                    success=True,
                )

                self.results.append(perf_result)
                self.log(f"âœ… {test_name}: {exec_time:.3f}s, å½±å“ {affected_rows} è¡Œ")
            else:
                self.log(
                    f"æ›´æ–°å¤±è´¥ {test_name}: {result.get('message', 'Unknown error')}",
                    "ERROR",
                )

    def benchmark_concurrent_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šå¹¶å‘æ€§èƒ½"""
        self.log("=== å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        def concurrent_operations(thread_id: int, operation_count: int) -> List[float]:
            times = []
            for i in range(operation_count):
                sql = f"INSERT INTO perf_index_test VALUES ({thread_id * 1000 + i}, {i % 100}, 'thread_{thread_id}_{i}', 'concurrent_test_data');"
                _, exec_time, success = self.execute_sql(sql)
                if success:
                    times.append(exec_time)
            return times

        # å¹¶å‘æ’å…¥æµ‹è¯•
        thread_counts = [1, 2, 4, 8]
        operations_per_thread = 50

        for num_threads in thread_counts:
            self.log(f"æµ‹è¯• {num_threads} ä¸ªå¹¶å‘çº¿ç¨‹...")

            with ThreadPoolExecutor(max_workers=num_threads) as executor:
                start_time = time.time()

                futures = []
                for thread_id in range(num_threads):
                    future = executor.submit(
                        concurrent_operations, thread_id, operations_per_thread
                    )
                    futures.append(future)

                all_times = []
                for future in as_completed(futures):
                    thread_times = future.result()
                    all_times.extend(thread_times)

                total_time = time.time() - start_time

            if all_times:
                total_operations = len(all_times)
                avg_time = statistics.mean(all_times)
                throughput = total_operations / total_time

                perf_result = PerformanceResult(
                    test_name=f"å¹¶å‘æ’å…¥_{num_threads}çº¿ç¨‹",
                    operation_type="CONCURRENT_INSERT",
                    record_count=total_operations,
                    execution_time=total_time,
                    throughput=throughput,
                    success=True,
                )

                self.results.append(perf_result)
                self.log(
                    f"âœ… {num_threads} çº¿ç¨‹: {total_operations} æ“ä½œ, {total_time:.3f}s, {throughput:.2f} ops/sec"
                )

    def benchmark_index_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šç´¢å¼•æ€§èƒ½"""
        self.log("=== ç´¢å¼•æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        # é¦–å…ˆå¡«å……ç´¢å¼•æµ‹è¯•è¡¨
        self.log("å¡«å……ç´¢å¼•æµ‹è¯•æ•°æ®...")
        test_data = self.generate_test_data("perf_index_test", 1000)

        batch_size = 100
        for i in range(0, len(test_data), batch_size):
            batch = test_data[i : i + batch_size]
            sql = f"INSERT INTO perf_index_test VALUES {', '.join(batch)};"
            self.execute_sql(sql)

        # æµ‹è¯•ç´¢å¼•æŸ¥è¯¢æ€§èƒ½
        index_queries = [
            (
                "ç­‰å€¼æŸ¥è¯¢_å¯èƒ½ä½¿ç”¨ç´¢å¼•",
                "SELECT * FROM perf_index_test WHERE pk_id = 500;",
            ),
            (
                "èŒƒå›´æŸ¥è¯¢_å¯èƒ½ä½¿ç”¨ç´¢å¼•",
                "SELECT * FROM perf_index_test WHERE pk_id BETWEEN 100 AND 200;",
            ),
            (
                "éç´¢å¼•åˆ—æŸ¥è¯¢",
                "SELECT * FROM perf_index_test WHERE search_field = 'search_050';",
            ),
            (
                "å¤åˆæ¡ä»¶æŸ¥è¯¢",
                "SELECT * FROM perf_index_test WHERE pk_id > 500 AND indexed_col < 100;",
            ),
        ]

        for test_name, sql in index_queries:
            times = []
            for _ in range(10):  # å¤šæ¬¡è¿è¡Œå–å¹³å‡
                result, exec_time, success = self.execute_sql(sql)
                if success:
                    times.append(exec_time)

            if times:
                avg_time = statistics.mean(times)
                throughput = 1 / avg_time if avg_time > 0 else 0

                perf_result = PerformanceResult(
                    test_name=test_name,
                    operation_type="INDEX_QUERY",
                    record_count=1,
                    execution_time=avg_time,
                    throughput=throughput,
                    success=True,
                )

                self.results.append(perf_result)
                self.log(f"âœ… {test_name}: å¹³å‡ {avg_time:.4f}s")

    def cleanup_test_data(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        self.log("æ¸…ç†æµ‹è¯•æ•°æ®...")

        tables = ["perf_small", "perf_medium", "perf_large", "perf_index_test"]
        for table in tables:
            result, _, success = self.execute_sql(f"DROP TABLE {table};")
            if success:
                self.log(f"âœ… åˆ é™¤è¡¨ {table}")
            else:
                self.log(f"âŒ åˆ é™¤è¡¨ {table} å¤±è´¥", "ERROR")

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        self.log("=== ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š ===")

        if not self.results:
            self.log("æ²¡æœ‰æ€§èƒ½æµ‹è¯•ç»“æœ", "WARNING")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š CS307æ•°æ®åº“æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„æŠ¥å‘Š
        operation_types = {}
        for result in self.results:
            op_type = result.operation_type
            if op_type not in operation_types:
                operation_types[op_type] = []
            operation_types[op_type].append(result)

        for op_type, results in operation_types.items():
            print(f"\nğŸ“ˆ {op_type} æ€§èƒ½:")
            print("-" * 60)

            for result in results:
                if result.success:
                    print(f"â€¢ {result.test_name}")
                    print(f"  æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}s")
                    print(f"  ååé‡: {result.throughput:.2f} ops/sec")
                    if result.record_count > 1:
                        print(f"  å¤„ç†è®°å½•: {result.record_count} æ¡")
                    print()

        # æ€§èƒ½ç»Ÿè®¡æ±‡æ€»
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        avg_throughput = statistics.mean(
            [r.throughput for r in self.results if r.success]
        )

        print(f"\nğŸ“‹ æ±‡æ€»ç»Ÿè®¡:")
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"å¹³å‡ååé‡: {avg_throughput:.2f} ops/sec")

        # æ‰¾å‡ºæ€§èƒ½æœ€å¥½å’Œæœ€å·®çš„æµ‹è¯•
        if successful_tests > 0:
            fastest = max(
                [r for r in self.results if r.success], key=lambda x: x.throughput
            )
            slowest = min(
                [r for r in self.results if r.success], key=lambda x: x.throughput
            )

            print(
                f"\nğŸ† æœ€å¿«æ“ä½œ: {fastest.test_name} ({fastest.throughput:.2f} ops/sec)"
            )
            print(
                f"ğŸŒ æœ€æ…¢æ“ä½œ: {slowest.test_name} ({slowest.throughput:.2f} ops/sec)"
            )

        self.save_performance_report()

    def save_performance_report(self):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_file = f"performance_report_{int(time.time())}.json"

        report_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total_tests": len(self.results),
                "successful_tests": sum(1 for r in self.results if r.success),
                "avg_throughput": (
                    statistics.mean([r.throughput for r in self.results if r.success])
                    if self.results
                    else 0
                ),
            },
            "results": [
                {
                    "test_name": r.test_name,
                    "operation_type": r.operation_type,
                    "record_count": r.record_count,
                    "execution_time": r.execution_time,
                    "throughput": r.throughput,
                    "success": r.success,
                    "error_message": r.error_message,
                }
                for r in self.results
            ],
        }

        try:
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            self.log(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            self.log(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}", "ERROR")

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        start_time = time.time()

        try:
            self.log("ğŸš€ å¼€å§‹CS307æ•°æ®åº“æ€§èƒ½åŸºå‡†æµ‹è¯•")

            # åˆ›å»ºæµ‹è¯•è¡¨
            if not self.create_test_tables():
                self.log("æ— æ³•åˆ›å»ºæµ‹è¯•è¡¨ï¼Œé€€å‡ºæµ‹è¯•", "ERROR")
                return

            # è¿è¡Œå„é¡¹æ€§èƒ½æµ‹è¯•
            self.benchmark_insert_performance()
            self.benchmark_select_performance()
            self.benchmark_update_performance()
            self.benchmark_concurrent_performance()
            self.benchmark_index_performance()

            # æ¸…ç†
            self.cleanup_test_data()

        except KeyboardInterrupt:
            self.log("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­", "WARNING")
        except Exception as e:
            self.log(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", "ERROR")
        finally:
            total_time = time.time() - start_time
            self.log(f"æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
            self.generate_performance_report()


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="CS307æ•°æ®åº“æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·")
    parser.add_argument(
        "--url", default="http://localhost:8080", help="æ•°æ®åº“HTTP APIåœ°å€"
    )
    parser.add_argument(
        "--test",
        choices=["insert", "select", "update", "concurrent", "index", "all"],
        default="all",
        help="æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•ç±»å‹",
    )

    args = parser.parse_args()

    benchmark = PerformanceBenchmark(args.url)

    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        result, _, success = benchmark.execute_sql("HELP;")
        if not success:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°æ•°æ®åº“: {args.url}")
            return
        print(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {args.url}")
    except Exception as e:
        print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return

    # åˆ›å»ºæµ‹è¯•è¡¨
    if not benchmark.create_test_tables():
        return

    # æ ¹æ®å‚æ•°è¿è¡ŒæŒ‡å®šæµ‹è¯•
    try:
        if args.test == "insert":
            benchmark.benchmark_insert_performance()
        elif args.test == "select":
            benchmark.benchmark_select_performance()
        elif args.test == "update":
            benchmark.benchmark_update_performance()
        elif args.test == "concurrent":
            benchmark.benchmark_concurrent_performance()
        elif args.test == "index":
            benchmark.benchmark_index_performance()
        else:
            benchmark.run_all_benchmarks()
    finally:
        benchmark.cleanup_test_data()
        benchmark.generate_performance_report()


if __name__ == "__main__":
    main()
